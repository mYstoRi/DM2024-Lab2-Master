{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bafa421-4d9b-4d3e-a7c6-d5c5c49b6919",
   "metadata": {},
   "source": [
    "## load data and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a0f2039-49c9-4d2a-aafc-4dc5c11dc33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb966f50-ec8e-488a-8d80-6ff10f41629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = 'dataset/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7448ff2e-9c68-4b5a-b245-b69be5f11dc3",
   "metadata": {},
   "source": [
    "loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b3e73a5-3327-47c5-9545-2a33356299b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_score</th>\n",
       "      <th>_index</th>\n",
       "      <th>_source</th>\n",
       "      <th>_crawldate</th>\n",
       "      <th>_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>391</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': ['Snapchat'], 'tweet_id...</td>\n",
       "      <td>2015-05-23 11:42:47</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>433</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': ['freepress', 'TrumpLeg...</td>\n",
       "      <td>2016-01-28 04:52:09</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>232</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': ['bibleverse'], 'tweet_...</td>\n",
       "      <td>2017-12-25 04:39:20</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>376</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': [], 'tweet_id': '0x1cd5...</td>\n",
       "      <td>2016-01-24 23:53:05</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>989</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': [], 'tweet_id': '0x2de2...</td>\n",
       "      <td>2016-01-08 17:18:59</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867530</th>\n",
       "      <td>827</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': ['mixedfeeling', 'butim...</td>\n",
       "      <td>2015-05-12 12:51:52</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867531</th>\n",
       "      <td>368</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': [], 'tweet_id': '0x29d0...</td>\n",
       "      <td>2017-10-02 17:54:04</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867532</th>\n",
       "      <td>498</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': [], 'tweet_id': '0x2a6a...</td>\n",
       "      <td>2016-10-10 11:04:32</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867533</th>\n",
       "      <td>840</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': [], 'tweet_id': '0x24fa...</td>\n",
       "      <td>2016-09-02 14:25:06</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867534</th>\n",
       "      <td>360</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': ['Sundayvibes'], 'tweet...</td>\n",
       "      <td>2016-11-16 01:40:07</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1867535 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         _score          _index  \\\n",
       "0           391  hashtag_tweets   \n",
       "1           433  hashtag_tweets   \n",
       "2           232  hashtag_tweets   \n",
       "3           376  hashtag_tweets   \n",
       "4           989  hashtag_tweets   \n",
       "...         ...             ...   \n",
       "1867530     827  hashtag_tweets   \n",
       "1867531     368  hashtag_tweets   \n",
       "1867532     498  hashtag_tweets   \n",
       "1867533     840  hashtag_tweets   \n",
       "1867534     360  hashtag_tweets   \n",
       "\n",
       "                                                   _source  \\\n",
       "0        {'tweet': {'hashtags': ['Snapchat'], 'tweet_id...   \n",
       "1        {'tweet': {'hashtags': ['freepress', 'TrumpLeg...   \n",
       "2        {'tweet': {'hashtags': ['bibleverse'], 'tweet_...   \n",
       "3        {'tweet': {'hashtags': [], 'tweet_id': '0x1cd5...   \n",
       "4        {'tweet': {'hashtags': [], 'tweet_id': '0x2de2...   \n",
       "...                                                    ...   \n",
       "1867530  {'tweet': {'hashtags': ['mixedfeeling', 'butim...   \n",
       "1867531  {'tweet': {'hashtags': [], 'tweet_id': '0x29d0...   \n",
       "1867532  {'tweet': {'hashtags': [], 'tweet_id': '0x2a6a...   \n",
       "1867533  {'tweet': {'hashtags': [], 'tweet_id': '0x24fa...   \n",
       "1867534  {'tweet': {'hashtags': ['Sundayvibes'], 'tweet...   \n",
       "\n",
       "                  _crawldate   _type  \n",
       "0        2015-05-23 11:42:47  tweets  \n",
       "1        2016-01-28 04:52:09  tweets  \n",
       "2        2017-12-25 04:39:20  tweets  \n",
       "3        2016-01-24 23:53:05  tweets  \n",
       "4        2016-01-08 17:18:59  tweets  \n",
       "...                      ...     ...  \n",
       "1867530  2015-05-12 12:51:52  tweets  \n",
       "1867531  2017-10-02 17:54:04  tweets  \n",
       "1867532  2016-10-10 11:04:32  tweets  \n",
       "1867533  2016-09-02 14:25:06  tweets  \n",
       "1867534  2016-11-16 01:40:07  tweets  \n",
       "\n",
       "[1867535 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text_df = pd.read_json(dataset_dir + 'tweets_DM.json', lines=True)\n",
    "all_text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c65db1c2-bcbd-4a7c-9103-1fcfe76a5e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "del all_text_df['_index']\n",
    "del all_text_df['_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce4be25d-b040-4350-a31f-9678da0319c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scraptext(source):\n",
    "    return source['tweet']['text']\n",
    "\n",
    "def scrapid(source):\n",
    "    return source['tweet']['tweet_id']\n",
    "\n",
    "all_text_df['text'] = all_text_df['_source'].apply(scraptext)\n",
    "all_text_df['id'] = all_text_df['_source'].apply(scrapid)\n",
    "del all_text_df['_source']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5e4be3-a3cf-4fcf-8b11-909bc45e201b",
   "metadata": {},
   "source": [
    "loading the train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2808a0f1-0c33-4cb6-8d39-8d8000e9e58e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = pd.read_csv(dataset_dir + 'data_identification.csv').set_index('tweet_id')['identification'].to_dict()\n",
    "split['0x28cc61']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f75000f-d708-4724-abe3-eb19b85b6222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_score</th>\n",
       "      <th>_crawldate</th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>391</td>\n",
       "      <td>2015-05-23 11:42:47</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>433</td>\n",
       "      <td>2016-01-28 04:52:09</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>232</td>\n",
       "      <td>2017-12-25 04:39:20</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "      <td>0x28b412</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>376</td>\n",
       "      <td>2016-01-24 23:53:05</td>\n",
       "      <td>Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;LH&gt;</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>989</td>\n",
       "      <td>2016-01-08 17:18:59</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "      <td>0x2de201</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867530</th>\n",
       "      <td>827</td>\n",
       "      <td>2015-05-12 12:51:52</td>\n",
       "      <td>When you buy the last 2 tickets remaining for ...</td>\n",
       "      <td>0x316b80</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867531</th>\n",
       "      <td>368</td>\n",
       "      <td>2017-10-02 17:54:04</td>\n",
       "      <td>I swear all this hard work gone pay off one da...</td>\n",
       "      <td>0x29d0cb</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867532</th>\n",
       "      <td>498</td>\n",
       "      <td>2016-10-10 11:04:32</td>\n",
       "      <td>@Parcel2Go no card left when I wasn't in so I ...</td>\n",
       "      <td>0x2a6a4f</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867533</th>\n",
       "      <td>840</td>\n",
       "      <td>2016-09-02 14:25:06</td>\n",
       "      <td>Ah, corporate life, where you can date &lt;LH&gt; us...</td>\n",
       "      <td>0x24faed</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867534</th>\n",
       "      <td>360</td>\n",
       "      <td>2016-11-16 01:40:07</td>\n",
       "      <td>Blessed to be living #Sundayvibes &lt;LH&gt;</td>\n",
       "      <td>0x34be8c</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1867535 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         _score           _crawldate  \\\n",
       "0           391  2015-05-23 11:42:47   \n",
       "1           433  2016-01-28 04:52:09   \n",
       "2           232  2017-12-25 04:39:20   \n",
       "3           376  2016-01-24 23:53:05   \n",
       "4           989  2016-01-08 17:18:59   \n",
       "...         ...                  ...   \n",
       "1867530     827  2015-05-12 12:51:52   \n",
       "1867531     368  2017-10-02 17:54:04   \n",
       "1867532     498  2016-10-10 11:04:32   \n",
       "1867533     840  2016-09-02 14:25:06   \n",
       "1867534     360  2016-11-16 01:40:07   \n",
       "\n",
       "                                                      text        id  split  \n",
       "0        People who post \"add me on #Snapchat\" must be ...  0x376b20  train  \n",
       "1        @brianklaas As we see, Trump is dangerous to #...  0x2d5350  train  \n",
       "2        Confident of your obedience, I write to you, k...  0x28b412   test  \n",
       "3                      Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <LH>  0x1cd5b0  train  \n",
       "4        \"Trust is not the same as faith. A friend is s...  0x2de201   test  \n",
       "...                                                    ...       ...    ...  \n",
       "1867530  When you buy the last 2 tickets remaining for ...  0x316b80   test  \n",
       "1867531  I swear all this hard work gone pay off one da...  0x29d0cb   test  \n",
       "1867532  @Parcel2Go no card left when I wasn't in so I ...  0x2a6a4f   test  \n",
       "1867533  Ah, corporate life, where you can date <LH> us...  0x24faed  train  \n",
       "1867534             Blessed to be living #Sundayvibes <LH>  0x34be8c  train  \n",
       "\n",
       "[1867535 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text_df['split'] = all_text_df['id'].map(split)\n",
    "all_text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d000fb9a-052c-4d00-9d57-70af7d69311e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training samples: 1455563\n",
      "testing samples: 411972\n"
     ]
    }
   ],
   "source": [
    "train_df = all_text_df[all_text_df['split'] == 'train']\n",
    "test_df = all_text_df[all_text_df['split'] == 'test']\n",
    "del all_text_df\n",
    "del train_df['split']\n",
    "del test_df['split']\n",
    "print(f'training samples: {len(train_df)}')\n",
    "print(f'testing samples: {len(test_df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6aa3e9-8a32-43ce-b415-4beae130faf6",
   "metadata": {},
   "source": [
    "then we append the label to the train df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06025b40-b089-4043-bb72-a671014a54c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_score</th>\n",
       "      <th>_crawldate</th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>391</td>\n",
       "      <td>2015-05-23 11:42:47</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>433</td>\n",
       "      <td>2016-01-28 04:52:09</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>376</td>\n",
       "      <td>2016-01-24 23:53:05</td>\n",
       "      <td>Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;LH&gt;</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>120</td>\n",
       "      <td>2015-06-11 04:44:05</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1021</td>\n",
       "      <td>2015-08-18 02:30:07</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867526</th>\n",
       "      <td>94</td>\n",
       "      <td>2016-12-26 02:44:07</td>\n",
       "      <td>I'm SO HAPPY!!! #NoWonder the name of this sho...</td>\n",
       "      <td>0x321566</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867527</th>\n",
       "      <td>627</td>\n",
       "      <td>2015-04-01 08:14:56</td>\n",
       "      <td>In every circumtance I'd like to be thankful t...</td>\n",
       "      <td>0x38959e</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867528</th>\n",
       "      <td>274</td>\n",
       "      <td>2016-11-17 23:46:22</td>\n",
       "      <td>there's currently two girls walking around the...</td>\n",
       "      <td>0x2cbca6</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867533</th>\n",
       "      <td>840</td>\n",
       "      <td>2016-09-02 14:25:06</td>\n",
       "      <td>Ah, corporate life, where you can date &lt;LH&gt; us...</td>\n",
       "      <td>0x24faed</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867534</th>\n",
       "      <td>360</td>\n",
       "      <td>2016-11-16 01:40:07</td>\n",
       "      <td>Blessed to be living #Sundayvibes &lt;LH&gt;</td>\n",
       "      <td>0x34be8c</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1455563 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         _score           _crawldate  \\\n",
       "0           391  2015-05-23 11:42:47   \n",
       "1           433  2016-01-28 04:52:09   \n",
       "3           376  2016-01-24 23:53:05   \n",
       "5           120  2015-06-11 04:44:05   \n",
       "6          1021  2015-08-18 02:30:07   \n",
       "...         ...                  ...   \n",
       "1867526      94  2016-12-26 02:44:07   \n",
       "1867527     627  2015-04-01 08:14:56   \n",
       "1867528     274  2016-11-17 23:46:22   \n",
       "1867533     840  2016-09-02 14:25:06   \n",
       "1867534     360  2016-11-16 01:40:07   \n",
       "\n",
       "                                                      text        id  \\\n",
       "0        People who post \"add me on #Snapchat\" must be ...  0x376b20   \n",
       "1        @brianklaas As we see, Trump is dangerous to #...  0x2d5350   \n",
       "3                      Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <LH>  0x1cd5b0   \n",
       "5        @RISKshow @TheKevinAllison Thx for the BEST TI...  0x1d755c   \n",
       "6             Still waiting on those supplies Liscus. <LH>  0x2c91a8   \n",
       "...                                                    ...       ...   \n",
       "1867526  I'm SO HAPPY!!! #NoWonder the name of this sho...  0x321566   \n",
       "1867527  In every circumtance I'd like to be thankful t...  0x38959e   \n",
       "1867528  there's currently two girls walking around the...  0x2cbca6   \n",
       "1867533  Ah, corporate life, where you can date <LH> us...  0x24faed   \n",
       "1867534             Blessed to be living #Sundayvibes <LH>  0x34be8c   \n",
       "\n",
       "              emotion  \n",
       "0        anticipation  \n",
       "1             sadness  \n",
       "3                fear  \n",
       "5                 joy  \n",
       "6        anticipation  \n",
       "...               ...  \n",
       "1867526           joy  \n",
       "1867527           joy  \n",
       "1867528           joy  \n",
       "1867533           joy  \n",
       "1867534           joy  \n",
       "\n",
       "[1455563 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.read_csv(dataset_dir + 'emotion.csv').set_index('tweet_id')['emotion'].to_dict()\n",
    "train_df['emotion'] = train_df['id'].map(labels)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6f8168-78f1-496a-82d2-e05355933da7",
   "metadata": {},
   "source": [
    "## classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9872accc-c97d-4206-86cd-a21605cc750a",
   "metadata": {},
   "source": [
    "w2v avg + linear layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed84e25a-75dd-45d6-a3a0-30e03d1c0c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\User\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([list(['People', 'who', 'post', '``', 'add', 'me', 'on', '#', 'Snapchat', \"''\", 'must', 'be', 'dehydrated', '.', 'Cuz', 'man', '....', 'that', \"'s\", '<', 'LH', '>']),\n",
       "       list(['@', 'brianklaas', 'As', 'we', 'see', ',', 'Trump', 'is', 'dangerous', 'to', '#', 'freepress', 'around', 'the', 'world', '.', 'What', 'a', '<', 'LH', '>', '<', 'LH', '>', '#', 'TrumpLegacy', '.', '#', 'CNN']),\n",
       "       list(['Now', 'ISSA', 'is', 'stalking', 'Tasha', 'ðŸ˜‚ðŸ˜‚ðŸ˜‚', '<', 'LH', '>'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "train_df['text_tokenized'] = train_df['text'].apply(lambda x: nltk.word_tokenize(x))\n",
    "train_df['token_length'] = train_df['text'].apply(lambda x: len(x))\n",
    "max_length = train_df['token_length'].max()\n",
    "print(max_length)\n",
    "\n",
    "training_corpus = train_df['text_tokenized'].values\n",
    "training_corpus[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64dc47f4-a22b-40f4-8e73-33979a7ceee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "## setting\n",
    "vector_dim = 100\n",
    "window_size = 5\n",
    "min_count = 1\n",
    "training_epochs = 20\n",
    "\n",
    "## model\n",
    "word2vec_model = Word2Vec(sentences=training_corpus, \n",
    "                          vector_size=vector_dim, window=window_size, \n",
    "                          min_count=min_count, epochs=training_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68c75f17-c1eb-49b2-8f8c-faea13ff7b24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.27016072,  0.33818942, -1.69212676, ...,  0.8048316 ,\n",
       "        -0.43175723, -0.18518403],\n",
       "       [-0.47214523,  0.52837126, -1.78387939, ...,  0.53324692,\n",
       "        -0.46946748,  0.15402299],\n",
       "       [ 0.83925252,  0.64547279, -0.83474266, ...,  0.61699959,\n",
       "        -0.40864921, -0.77180999],\n",
       "       ...,\n",
       "       [-0.3635265 ,  0.27385815, -0.56093014, ..., -0.08497208,\n",
       "        -0.33685933, -0.10370188],\n",
       "       [-0.60327923,  0.5822662 , -1.13055495, ...,  0.57257048,\n",
       "        -0.89749688,  0.43787144],\n",
       "       [-0.52877387,  0.89923137, -1.99219664, ...,  0.77621801,\n",
       "        -0.41572634, -0.37786328]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sentence_embedding(tokens):\n",
    "    s = np.zeros(vector_dim)\n",
    "    for t in tokens:\n",
    "        if t in word2vec_model.wv:\n",
    "            s += word2vec_model.wv[t]\n",
    "    s /= len(tokens)\n",
    "    return s\n",
    "\n",
    "X_train = train_df['text_tokenized'].apply(sentence_embedding)\n",
    "X_train = np.stack(X_train.values)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69d1810b-b751-435b-881d-63d07d69ab1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "test_df['text_tokenized'] = test_df['text'].apply(lambda x: nltk.word_tokenize(x))\n",
    "X_test = test_df['text_tokenized'].apply(sentence_embedding)\n",
    "X_test = np.stack(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7ee6e89e-6ec1-4770-bd63-b69cbddb15f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1455563, 100)\n",
      "(1455563, 8)\n",
      "(411972, 100)\n",
      "(411972, 8)\n"
     ]
    }
   ],
   "source": [
    "y_train = pd.get_dummies(train_df['emotion'])\n",
    "y_test = np.zeros((X_test.shape[0], 8))\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d474d27f",
   "metadata": {},
   "source": [
    "train/test generators to help with limited memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ebf8d2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "batch_size = 16\n",
    "learning_rate = 0.01 # not functional atm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ea306e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return batch_x, batch_y\n",
    "\n",
    "train_gen = DataGenerator(X_train, y_train, batch_size)\n",
    "test_gen = DataGenerator(X_test, y_test, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "43a4fa8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.27016072  0.33818942 -1.69212676 ...  0.8048316  -0.43175723\n",
      "  -0.18518403]\n",
      " [-0.47214523  0.52837126 -1.78387939 ...  0.53324692 -0.46946748\n",
      "   0.15402299]\n",
      " [ 0.83925252  0.64547279 -0.83474266 ...  0.61699959 -0.40864921\n",
      "  -0.77180999]\n",
      " ...\n",
      " [-0.58194495  0.79526537 -0.79201595 ...  0.59906261 -0.76263964\n",
      "   0.1911658 ]\n",
      " [-0.08171081  0.65802381 -0.89767754 ... -0.22589033 -0.81494005\n",
      "   0.37768428]\n",
      " [-0.2602329  -0.4984562  -0.73629155 ...  0.29570149 -0.19513316\n",
      "   0.14786392]]\n",
      "    anger  anticipation  disgust  fear  joy  sadness  surprise  trust\n",
      "0       0             1        0     0    0        0         0      0\n",
      "1       0             0        0     0    0        1         0      0\n",
      "3       0             0        0     1    0        0         0      0\n",
      "5       0             0        0     0    1        0         0      0\n",
      "6       0             1        0     0    0        0         0      0\n",
      "7       0             0        0     0    1        0         0      0\n",
      "8       0             0        0     0    0        1         0      0\n",
      "10      0             1        0     0    0        0         0      0\n",
      "11      0             0        0     0    1        0         0      0\n",
      "12      1             0        0     0    0        0         0      0\n",
      "13      0             0        0     0    1        0         0      0\n",
      "14      1             0        0     0    0        0         0      0\n",
      "15      0             0        0     1    0        0         0      0\n",
      "16      0             1        0     0    0        0         0      0\n",
      "17      0             0        0     0    1        0         0      0\n",
      "18      0             0        0     0    0        0         0      1\n"
     ]
    }
   ],
   "source": [
    "for test1, test2 in train_gen:\n",
    "    print(test1)\n",
    "    print(test2)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38281ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b418b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2cee56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d31f88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2f26d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c967ef0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8276a030-a89d-4ee7-a037-012b0a8d03c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cf515d0d-2405-4329-b35f-89d9b35ff432",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', kernel_initializer=HeNormal()))\n",
    "model.add(Dense(64, activation='relu', kernel_initializer=HeNormal()))\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "151e4a9a-ff58-43c8-893a-821bab4f5e57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "90973/90973 [==============================] - 253s 3ms/step - loss: 1.3630 - accuracy: 0.5015\n",
      "Epoch 2/25\n",
      "90973/90973 [==============================] - 251s 3ms/step - loss: 1.3250 - accuracy: 0.5171\n",
      "Epoch 3/25\n",
      "90973/90973 [==============================] - 252s 3ms/step - loss: 1.3148 - accuracy: 0.5211\n",
      "Epoch 4/25\n",
      "90973/90973 [==============================] - 251s 3ms/step - loss: 1.3096 - accuracy: 0.5236\n",
      "Epoch 5/25\n",
      "90973/90973 [==============================] - 252s 3ms/step - loss: 1.3063 - accuracy: 0.5249\n",
      "Epoch 6/25\n",
      "90973/90973 [==============================] - 253s 3ms/step - loss: 1.3042 - accuracy: 0.5262\n",
      "Epoch 7/25\n",
      "90973/90973 [==============================] - 251s 3ms/step - loss: 1.3035 - accuracy: 0.5271\n",
      "Epoch 8/25\n",
      "90973/90973 [==============================] - 252s 3ms/step - loss: 1.3012 - accuracy: 0.5279\n",
      "Epoch 9/25\n",
      "90973/90973 [==============================] - 253s 3ms/step - loss: 1.2994 - accuracy: 0.5285\n",
      "Epoch 10/25\n",
      "90973/90973 [==============================] - 254s 3ms/step - loss: 1.2973 - accuracy: 0.5293\n",
      "Epoch 11/25\n",
      "90973/90973 [==============================] - 252s 3ms/step - loss: 1.2961 - accuracy: 0.5299\n",
      "Epoch 12/25\n",
      "90973/90973 [==============================] - 252s 3ms/step - loss: 1.2955 - accuracy: 0.5301\n",
      "Epoch 13/25\n",
      "90973/90973 [==============================] - 252s 3ms/step - loss: 1.2934 - accuracy: 0.5310\n",
      "Epoch 14/25\n",
      "90973/90973 [==============================] - 249s 3ms/step - loss: 1.2926 - accuracy: 0.5313\n",
      "Epoch 15/25\n",
      "90973/90973 [==============================] - 245s 3ms/step - loss: 1.2908 - accuracy: 0.5323\n",
      "Epoch 16/25\n",
      "90973/90973 [==============================] - 243s 3ms/step - loss: 1.2911 - accuracy: 0.5325\n",
      "Epoch 17/25\n",
      "90973/90973 [==============================] - 235s 3ms/step - loss: 1.2895 - accuracy: 0.5330\n",
      "Epoch 18/25\n",
      "90973/90973 [==============================] - 234s 3ms/step - loss: 1.2893 - accuracy: 0.5330\n",
      "Epoch 19/25\n",
      "90973/90973 [==============================] - 233s 3ms/step - loss: 1.2890 - accuracy: 0.5337\n",
      "Epoch 20/25\n",
      "90973/90973 [==============================] - 233s 3ms/step - loss: 1.2873 - accuracy: 0.5337\n",
      "Epoch 21/25\n",
      "90973/90973 [==============================] - 233s 3ms/step - loss: 1.2876 - accuracy: 0.5338\n",
      "Epoch 22/25\n",
      "90973/90973 [==============================] - 233s 3ms/step - loss: 1.2873 - accuracy: 0.5341\n",
      "Epoch 23/25\n",
      "90973/90973 [==============================] - 234s 3ms/step - loss: 1.2864 - accuracy: 0.5342\n",
      "Epoch 24/25\n",
      "90973/90973 [==============================] - 234s 3ms/step - loss: 1.2862 - accuracy: 0.5349\n",
      "Epoch 25/25\n",
      "90973/90973 [==============================] - 233s 3ms/step - loss: 1.2849 - accuracy: 0.5352\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cef375c320>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_gen, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "216df196-7dbb-4cfc-bf11-e5dd7e5d697f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25749/25749 [==============================] - 28s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[9.8100118e-04, 8.9025015e-01, 9.0941042e-04, ..., 3.0529734e-03,\n",
       "        5.7068473e-04, 2.2451924e-02],\n",
       "       [2.5951457e-03, 7.1532816e-01, 1.2567798e-03, ..., 4.0807910e-03,\n",
       "        1.0000638e-03, 7.8386992e-02],\n",
       "       [1.4934844e-02, 3.5782865e-01, 3.1886961e-02, ..., 1.8204361e-01,\n",
       "        5.2397572e-02, 8.3831750e-02],\n",
       "       ...,\n",
       "       [3.1465594e-02, 4.8731565e-02, 2.0320472e-01, ..., 4.4760394e-01,\n",
       "        9.8243900e-02, 3.1843487e-02],\n",
       "       [4.8674464e-02, 3.0874630e-02, 8.2688957e-02, ..., 1.6228706e-01,\n",
       "        3.0085878e-02, 1.3296527e-01],\n",
       "       [1.1981687e-01, 8.9540491e-03, 2.1950313e-01, ..., 3.7696031e-01,\n",
       "        4.8355576e-02, 4.0536754e-02]], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(test_gen)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "aa4ba736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x28b412</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2de201</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x218443</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2939d5</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x26289a</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411967</th>\n",
       "      <td>0x2913b4</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411968</th>\n",
       "      <td>0x2a980e</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411969</th>\n",
       "      <td>0x316b80</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411970</th>\n",
       "      <td>0x29d0cb</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411971</th>\n",
       "      <td>0x2a6a4f</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>411972 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id       emotion\n",
       "0       0x28b412  anticipation\n",
       "1       0x2de201  anticipation\n",
       "2       0x218443  anticipation\n",
       "3       0x2939d5           joy\n",
       "4       0x26289a  anticipation\n",
       "...          ...           ...\n",
       "411967  0x2913b4  anticipation\n",
       "411968  0x2a980e  anticipation\n",
       "411969  0x316b80       sadness\n",
       "411970  0x29d0cb           joy\n",
       "411971  0x2a6a4f       sadness\n",
       "\n",
       "[411972 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_df = pd.DataFrame()\n",
    "output_df['id'] = test_df['id']\n",
    "output_df = output_df.reset_index()\n",
    "del output_df['index']\n",
    "output_df['emotion'] = pd.DataFrame(y_pred, columns=y_train.columns).idxmax(axis=1)\n",
    "display(output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "59aa2c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.to_csv('prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ed3cef-bdfd-4996-8947-0d9baf5a9a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a2ad6a-03dd-4601-93d8-6895256f567f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be737439-8d61-4cef-9055-c3c2e8c9b75d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816e758d-4f6e-4444-84f6-ee354172590b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca48152c-10dd-43dc-a822-fa417f8de0b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ca3a67-3575-4ac8-b2b6-1c67f3089470",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734e3b16-b13b-45ee-92af-8528980b7d96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874a85a2-ab0c-4452-b007-d87c3743180d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57939f38-04bb-4e44-8737-edaf655767ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102b742b-a37f-45b5-bf33-0c2be93eca18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274cb1b1-0383-4736-8296-bee239282dc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
